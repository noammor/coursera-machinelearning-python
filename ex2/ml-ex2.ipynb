{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Exercise 2: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "import scipy.optimize\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Plotting \n",
    "  We start the exercise by first plotting the data to understand the \n",
    "  the problem we are working with.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data1 = pandas.read_csv(\"ex2data1.txt\", header=None, names=['test1', 'test2', 'accepted'])\n",
    "data1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting data with + indicating (y = 1) examples and o \n",
    " indicating (y = 0) examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plotData(data):\n",
    "    fig, ax = plt.subplots()\n",
    "    results_accepted = data[data.accepted == 1]\n",
    "    results_rejected = data[data.accepted == 0]\n",
    "    ax.scatter(results_accepted.test1, results_accepted.test2, marker='+', c='b', s=40)\n",
    "    ax.scatter(results_rejected.test1, results_rejected.test2, marker='o', c='r', s=30)\n",
    "    return ax\n",
    "\n",
    "ax = plotData(data1)\n",
    "ax.set_ylim([20, 130])\n",
    "ax.legend(['Admitted', 'Not admitted'], loc='best')\n",
    "ax.set_xlabel('Exam 1 score')\n",
    "ax.set_ylabel('Exam 2 score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = data1[['test1', 'test2']].values\n",
    "y = data1.accepted.values\n",
    "m, n = X.shape\n",
    "X = np.insert(X, 0, np.ones(len(X)), 1)\n",
    "m, n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Compute Cost and Gradient \n",
    "  In this part of the exercise, you will implement the cost and gradient\n",
    "  for logistic regression. You neeed to complete the code in \n",
    "  the function `cost`.\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    #SIGMOID Compute sigmoid functoon\n",
    "    #   J = SIGMOID(z) computes the sigmoid of z.\n",
    "    \n",
    "    # You need to return the following variables correctly \n",
    "    g = np.zeros(z.shape)\n",
    "\n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "    # Instructions: Compute the sigmoid of each value of z (z can be a matrix,\n",
    "    #               vector or scalar).\n",
    "\n",
    "    \n",
    "    # =============================================================\n",
    "    \n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cost(X, y, theta, lambda_=0):\n",
    "    #COSTFUNCTION Compute cost and gradient for logistic regression\n",
    "    #   J = COSTFUNCTION(theta, X, y) computes the cost of using theta as the\n",
    "    #   parameter for logistic regression and the gradient of the cost\n",
    "    #   w.r.t. to the parameters.\n",
    "\n",
    "    # Initialize some useful values\n",
    "    m = len(y)\n",
    "    \n",
    "    # You need to return the following variables correctly\n",
    "    J = 0\n",
    "    \n",
    "    \n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "    # Instructions: Compute the cost of a particular choice of theta.\n",
    "    #               You should set J to the cost.\n",
    "    #               Compute the partial derivatives and set grad to the partial\n",
    "    #               derivatives of the cost w.r.t. each parameter in theta\n",
    "    #\n",
    "\n",
    "    \n",
    "    \n",
    "    # =============================================================\n",
    "    \n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gradient(X, y, theta, lambda_=0):\n",
    "    # Initialize some useful values\n",
    "    m = len(y)\n",
    "    \n",
    "    # You need to return the following variables correctly\n",
    "    grad = np.zeros(theta.shape)\n",
    "    \n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # =============================================================\n",
    "    \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "initial_theta = np.zeros(n + 1)\n",
    "initial_theta.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cost at initial theta (zeros) should be about `0.693`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cost(X, y, np.array(initial_theta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient at initial theta should be `[-0.1, -12.01, -11.26]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gradient(X, y, np.array([0,0,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Optimizing using fminunc\n",
    "  In this exercise, you will use a built-in function (scipy.optimize.fmin_ncg) to find the\n",
    "  optimal parameters theta.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mycost(t):\n",
    "    return cost(X, y, t)\n",
    "\n",
    "def mygrad(t):\n",
    "    return gradient(X, y, t)\n",
    "\n",
    "optimal_theta = scipy.optimize.fmin_ncg(mycost,\n",
    "                                        initial_theta,\n",
    "                                        fprime=mygrad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Value of theta that minimizes the cost function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimal_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ax = plotData(data1)\n",
    "x_plot = np.array([np.max(X[:, 1]), np.min(X[:,1])])\n",
    "y_plot = (-optimal_theta[0] - optimal_theta[1]*x_plot) / (optimal_theta[2])\n",
    "ax.plot(x_plot, y_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Predict and Accuracies \n",
    "  After learning the parameters, you'll like to use it to predict the outcomes\n",
    "  on unseen data. In this part, you will use the logistic regression model\n",
    "  to predict the probability that a student with score 45 on exam 1 and \n",
    "  score 85 on exam 2 will be admitted.\n",
    "\n",
    "  Furthermore, you will compute the training and test set accuracies of \n",
    "  our model.\n",
    "\n",
    "  Your task is to complete the code in `predict`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(t, x):\n",
    "    #PREDICT Predict whether the label is 0 or 1 using learned logistic \n",
    "    #regression parameters theta\n",
    "    #   p = PREDICT(theta, X) computes the predictions for X using a \n",
    "    #   threshold at 0.5 (i.e., if sigmoid(theta'*x) >= 0.5, predict 1)\n",
    "    \n",
    "    m = X.shape[0] # Number of training examples\n",
    "    \n",
    "    # You need to return the following variables correctly\n",
    "    p = np.zeros(m)\n",
    "    \n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "    # Instructions: Complete the following code to make predictions using\n",
    "    #               your learned logistic regression parameters. \n",
    "    #               You should set p to a vector of 0's and 1's\n",
    "    #\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    # =========================================================================\n",
    "    \n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's predict the admission probably of a student with scores 45 and 85:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training set accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.mean(predict(optimal_theta, X) == y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Regularized logistic regression\n",
    "\n",
    "In this part, you are given a dataset with data points that are not\n",
    "linearly separable. However, you would still like to use logistic \n",
    "regression to classify the data points. \n",
    "\n",
    "To do so, you introduce more features to use -- in particular, you add\n",
    "polynomial features to our data matrix (similar to polynomial\n",
    "regression).\n",
    "\n",
    "You're expected to modify the cost and gradient functions you've already written so that they take the regularization constant into account and perform regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data2 = pandas.read_csv(\"./ex2data2.txt\", header=None, names=['test1', 'test2', 'accepted'])\n",
    "data2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ax = plotData(data2)\n",
    "ax.legend(['y = 1', 'y = 0'], loc='best')\n",
    "ax.set_xlabel('Microchip test 1')\n",
    "ax.set_ylabel('Microchip test 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mapFeature(x1, x2):\n",
    "    ret = np.array([x1**(i-j) * x2**j \n",
    "                    for i in range(1,7) for j in range(i+1)\n",
    "                   ])\n",
    "    return np.insert(ret, 0, np.ones(len(x1)), 0).T\n",
    "\n",
    "mapFeature(np.array([2,3]),np.array([3,2]))[:, :10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Note that mapFeature also adds a column of ones for us, so the intercept\n",
    " term is handled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = mapFeature(data2.test1, data2.test2)\n",
    "y = data2.accepted.values\n",
    "initial_theta = np.zeros(X.shape[1])\n",
    "X.shape, y.shape, initial_theta.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cost at the initial theta is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cost(X, y, initial_theta, lambda_) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Regularization and Accuracies \n",
    "  Optional Exercise:\n",
    " In this part, you will get to try different values of lambda and \n",
    "  see how regularization affects the decision coundart\n",
    "\n",
    "  Try the following values of lambda (0, 1, 10, 100).\n",
    "\n",
    "  How does the decision boundary change when you vary lambda? How does\n",
    "  the training set accuracy vary?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lambda_ = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimal_theta = scipy.optimize.fmin_bfgs(lambda t: cost(X, y, t, lambda_),\n",
    "                                        initial_theta,\n",
    "                                        lambda t: gradient(X, y, t, lambda_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the optimal theta value, the accuracy is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.mean(predict(optimal_theta, X) == y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimal_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision boundary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "contour_x = np.linspace(-1, 1.5)\n",
    "contour_y = np.linspace(-1, 1.5)\n",
    "def calc_z(x, y):\n",
    "    return mapFeature(np.array([x]), np.array([y])).dot(optimal_theta)\n",
    "\n",
    "z = np.zeros((len(contour_x), len(contour_y)))\n",
    "for i, c_x in enumerate(contour_x):\n",
    "    for j, c_y in enumerate(contour_y):\n",
    "        z[i,j] = calc_z(c_x, c_y)[0]\n",
    "        \n",
    "ax = plotData(data2)\n",
    "ax.contour(contour_x, contour_y, z, levels=[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
